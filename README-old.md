# ðŸš€ Azure Data Factory Project

## ðŸŽ¯ Project Overview

**Copy a text file from Azure Blob Storage (Source) to another Blob container (Sink)** using **Azure Data Factory**.

This project demonstrates two approaches:
1. **ðŸ–±ï¸ Manual Setup** - Step-by-step Azure Portal configuration
2. **ðŸ¤– Infrastructure as Code** - Automated Terraform deployment

---

## ðŸ—ï¸ Architecture

```
[Source Container]  â”€â”€â–º  [Azure Data Factory]  â”€â”€â–º  [Destination Container]
     input.txt                 Copy Activity               output.txt
```

---

## âœ… Prerequisites

### For Manual Setup:
* âœ… Azure Subscription
* âœ… Azure CLI installed and configured (`az login`)
* âœ… Basic knowledge of Azure Portal

### For Terraform Setup:
* âœ… Azure Subscription
* âœ… Azure CLI installed and configured (`az login`)
* âœ… Terraform installed (v1.0+)

---

# ðŸ–±ï¸ Method 1: Manual Setup (Step-by-Step)

## ðŸ”¹ Step 1: Create Resource Group

```bash
az group create \
  --name adf-rg \
  --location eastus
```

## ðŸ”¹ Step 2: Create Storage Account

```bash
az storage account create \
  --name adfstoragedemo123 \
  --resource-group adf-rg \
  --location eastus \
  --sku Standard_LRS
```

## ðŸ”¹ Step 3: Create Storage Containers

```bash
# Create source container
az storage container create \
  --account-name adfstoragedemo123 \
  --name source

# Create destination container
az storage container create \
  --account-name adfstoragedemo123 \
  --name destination
```

## ðŸ”¹ Step 4: Create and Upload Sample File

```bash
# Create sample file
echo "Hello World
Sample data for Azure Data Factory demo" > input.txt

# Upload to source container
az storage blob upload \
  --account-name adfstoragedemo123 \
  --container-name source \
  --name input.txt \
  --file input.txt
```

## ðŸ”¹ Step 5: Install Data Factory Extension & Create ADF

```bash
# Install Azure Data Factory CLI extension
az extension add --name datafactory

# Create Azure Data Factory
az datafactory create \
  --resource-group adf-rg \
  --factory-name adf-simple-demo \
  --location eastus
```

## ðŸ”¹ Step 6: Configure Data Factory (Azure Portal)

1. **Open Azure Portal** â†’ **Data Factories** â†’ `adf-simple-demo`
2. **Click "Open Azure Data Factory Studio"**

## ðŸ”¹ Step 7: Create Linked Service

**Navigation:** Author â†’ Manage â†’ Linked Services â†’ New

**Configuration:**
- **Type:** Azure Blob Storage
- **Name:** AzureBlobStorage1
- **Connection Method:** Connection String

## ðŸ”¹ Step 7: Create Linked Service (Blob Storage)

### Linked Service JSON

```json
{
  "properties": {
    "type": "AzureBlobStorage",
    "typeProperties": {
      "connectionString": "DefaultEndpointsProtocol=https;AccountName=adfstoragedemo123;AccountKey=<KEY>;EndpointSuffix=core.windows.net"
    }
  }
}
```

---

## ðŸ”¹ Step 8: Create Datasets

### Source Dataset
**Navigation:** Author â†’ Datasets â†’ New Dataset â†’ Azure Blob Storage â†’ DelimitedText

**Configuration:**
- **Name:** SourceDataset
- **Linked Service:** AzureBlobStorage1
- **File Path:** Container: `source`, File: `input.txt`

**JSON:**
```json
{
  "properties": {
    "linkedServiceName": {
      "referenceName": "AzureBlobStorage1",
      "type": "LinkedServiceReference"
    },
    "type": "DelimitedText",
    "typeProperties": {
      "location": {
        "type": "AzureBlobStorageLocation",
        "container": "source",
        "fileName": "input.txt"
      }
    }
  }
}
```

### Sink Dataset
**Configuration:**
- **Name:** SinkDataset
- **Linked Service:** AzureBlobStorage1
- **File Path:** Container: `destination`, File: `output.txt`

**JSON:**
```json
{
  "properties": {
    "linkedServiceName": {
      "referenceName": "AzureBlobStorage1",
      "type": "LinkedServiceReference"
    },
    "type": "DelimitedText",
    "typeProperties": {
      "location": {
        "type": "AzureBlobStorageLocation",
        "container": "destination",
        "fileName": "output.txt"
      }
    }
  }
}
```

---

## ðŸ”¹ Step 9: Create Pipeline

**Navigation:** Author â†’ Pipelines â†’ New Pipeline

**Configuration:**
- **Name:** CopyBlobPipeline
- **Activity:** Copy Data
- **Source:** SourceDataset
- **Sink:** SinkDataset

**JSON:**
```json
{
  "name": "CopyBlobPipeline",
  "properties": {
    "activities": [
      {
        "name": "CopyFromSourceToDestination",
        "type": "Copy",
        "inputs": [
          {
            "referenceName": "SourceDataset",
            "type": "DatasetReference"
          }
        ],
        "outputs": [
          {
            "referenceName": "SinkDataset",
            "type": "DatasetReference"
          }
        ],
        "typeProperties": {
          "source": {
            "type": "DelimitedTextSource"
          },
          "sink": {
            "type": "DelimitedTextSink"
          }
        }
      }
    ]
  }
}
```

## ðŸ”¹ Step 10: Validate & Publish

1. âœ… Click **Validate All** to check for errors
2. âœ… Click **Publish** to deploy changes

## ðŸ”¹ Step 11: Test Pipeline

1. ðŸ‘‰ Click **Add Trigger â†’ Trigger Now**
2. ðŸ“Š Monitor the run in **Monitor â†’ Pipeline Runs**

## ðŸŽ‰ Verify Results

```bash
# Check if output file was created
az storage blob list \
  --account-name adfstoragedemo123 \
  --container-name destination \
  --output table

# Download and view the output file
az storage blob download \
  --account-name adfstoragedemo123 \
  --container-name destination \
  --name output.txt \
  --file output.txt && cat output.txt
```

---

## ï¿½ Infrastructure as Code (Terraform)

This project now includes **Terraform Infrastructure as Code** for automated deployment!

### ðŸš€ Quick Deploy with Terraform

```bash
cd terraform/
terraform init
terraform plan
terraform apply
```

### ðŸŽ¯ What Terraform Creates

- âœ… Resource Group (`adf-rg`)
- âœ… Storage Account (`adfstoragedemo123`)
- âœ… Blob Containers (`source`, `destination`)
- âœ… Azure Data Factory (`adf-simple-demo`)
- âœ… Linked Service (Blob Storage connection)
- âœ… Datasets (Source and Sink for delimited text)
- âœ… Pipeline (Copy activity from source to destination)
- âœ… Scheduled Trigger (Daily at 9 AM, disabled by default)
- âœ… Sample Data (`input.txt` automatically uploaded)

### ðŸ“ Terraform Structure

```
terraform/
â”œâ”€â”€ main.tf              # Main infrastructure configuration
â”œâ”€â”€ variables.tf         # Input variables
â”œâ”€â”€ outputs.tf          # Output values
â”œâ”€â”€ terraform.tfvars.example  # Example variables
â”œâ”€â”€ deploy.sh           # Automated deployment script
â”œâ”€â”€ README.md           # Terraform documentation
â””â”€â”€ .gitignore         # Terraform-specific gitignore
```

### ðŸ§ª Test the Pipeline

```bash
# Trigger pipeline run
az datafactory pipeline create-run \
  --factory-name adf-simple-demo \
  --resource-group adf-rg \
  --name CopyBlobPipeline

# Check results
az storage blob list \
  --account-name adfstoragedemo123 \
  --container-name destination \
  --output table
```

---

## ï¿½ðŸ“Œ Interview / Training One-Line Explanation

> *Azure Data Factory is a cloud-based ETL service used to move and transform data between different data sources using pipelines.*

