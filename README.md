# ğŸš€ Azure Data Factory Project

## ğŸ¯ Project Overview

**Copy a text file from Azure Blob Storage (Source) to another Blob container (Sink)** using **Azure Data Factory**.

This project demonstrates two approaches:
1. **ğŸ–±ï¸ Manual Setup** - Step-by-step Azure Portal configuration
2. **ğŸ¤– Infrastructure as Code** - Automated Terraform deployment

---

## ğŸ—ï¸ Architecture

```
[Source Container]  â”€â”€â–º  [Azure Data Factory]  â”€â”€â–º  [Destination Container]
     input.txt                 Copy Activity               output.txt
```

---

## âœ… Prerequisites

### For Manual Setup:
* âœ… Azure Subscription
* âœ… Azure CLI installed and configured (`az login`)
* âœ… Basic knowledge of Azure Portal

### For Terraform Setup:
* âœ… Azure Subscription
* âœ… Azure CLI installed and configured (`az login`)
* âœ… Terraform installed (v1.0+)

---

# ğŸ–±ï¸ Method 1: Manual Setup (Step-by-Step)

## ğŸ”¹ Step 1: Create Resource Group

```bash
az group create \
  --name adf-rg \
  --location eastus
```

## ğŸ”¹ Step 2: Create Storage Account

```bash
az storage account create \
  --name adfstoragedemo123 \
  --resource-group adf-rg \
  --location eastus \
  --sku Standard_LRS
```

## ğŸ”¹ Step 3: Create Storage Containers

```bash
# Create source container
az storage container create \
  --account-name adfstoragedemo123 \
  --name source

# Create destination container
az storage container create \
  --account-name adfstoragedemo123 \
  --name destination
```

## ğŸ”¹ Step 4: Create and Upload Sample File

```bash
# Create sample file
echo "Hello World
Sample data for Azure Data Factory demo" > input.txt

# Upload to source container
az storage blob upload \
  --account-name adfstoragedemo123 \
  --container-name source \
  --name input.txt \
  --file input.txt
```

## ğŸ”¹ Step 5: Install Data Factory Extension & Create ADF

```bash
# Install Azure Data Factory CLI extension
az extension add --name datafactory

# Create Azure Data Factory
az datafactory create \
  --resource-group adf-rg \
  --factory-name adf-simple-demo \
  --location eastus
```

## ğŸ”¹ Step 6: Configure Data Factory (Azure Portal)

1. **Open Azure Portal** â†’ **Data Factories** â†’ `adf-simple-demo`
2. **Click "Open Azure Data Factory Studio"**

## ğŸ”¹ Step 7: Create Linked Service

**Navigation:** Author â†’ Manage â†’ Linked Services â†’ New

**Configuration:**
- **Type:** Azure Blob Storage
- **Name:** AzureBlobStorage1
- **Connection Method:** Connection String

**JSON Configuration:**

```json
{
  "properties": {
    "type": "AzureBlobStorage",
    "typeProperties": {
      "connectionString": "DefaultEndpointsProtocol=https;AccountName=adfstoragedemo123;AccountKey=<KEY>;EndpointSuffix=core.windows.net"
    }
  }
}
```

## ğŸ”¹ Step 8: Create Datasets

### Source Dataset
**Navigation:** Author â†’ Datasets â†’ New Dataset â†’ Azure Blob Storage â†’ DelimitedText

**Configuration:**
- **Name:** SourceDataset
- **Linked Service:** AzureBlobStorage1
- **File Path:** Container: `source`, File: `input.txt`

**JSON:**
```json
{
  "properties": {
    "linkedServiceName": {
      "referenceName": "AzureBlobStorage1",
      "type": "LinkedServiceReference"
    },
    "type": "DelimitedText",
    "typeProperties": {
      "location": {
        "type": "AzureBlobStorageLocation",
        "container": "source",
        "fileName": "input.txt"
      }
    }
  }
}
```

### Sink Dataset
**Configuration:**
- **Name:** SinkDataset
- **Linked Service:** AzureBlobStorage1
- **File Path:** Container: `destination`, File: `output.txt`

**JSON:**
```json
{
  "properties": {
    "linkedServiceName": {
      "referenceName": "AzureBlobStorage1",
      "type": "LinkedServiceReference"
    },
    "type": "DelimitedText",
    "typeProperties": {
      "location": {
        "type": "AzureBlobStorageLocation",
        "container": "destination",
        "fileName": "output.txt"
      }
    }
  }
}
```

## ğŸ”¹ Step 9: Create Pipeline

**Navigation:** Author â†’ Pipelines â†’ New Pipeline

**Configuration:**
- **Name:** CopyBlobPipeline
- **Activity:** Copy Data
- **Source:** SourceDataset
- **Sink:** SinkDataset

**JSON:**
```json
{
  "name": "CopyBlobPipeline",
  "properties": {
    "activities": [
      {
        "name": "CopyFromSourceToDestination",
        "type": "Copy",
        "inputs": [
          {
            "referenceName": "SourceDataset",
            "type": "DatasetReference"
          }
        ],
        "outputs": [
          {
            "referenceName": "SinkDataset",
            "type": "DatasetReference"
          }
        ],
        "typeProperties": {
          "source": {
            "type": "DelimitedTextSource"
          },
          "sink": {
            "type": "DelimitedTextSink"
          }
        }
      }
    ]
  }
}
```

## ğŸ”¹ Step 10: Validate & Publish

1. âœ… Click **Validate All** to check for errors
2. âœ… Click **Publish** to deploy changes

## ğŸ”¹ Step 11: Test Pipeline

1. ğŸ‘‰ Click **Add Trigger â†’ Trigger Now**
2. ğŸ“Š Monitor the run in **Monitor â†’ Pipeline Runs**

## ğŸ‰ Verify Results

```bash
# Check if output file was created
az storage blob list \
  --account-name adfstoragedemo123 \
  --container-name destination \
  --output table

# Download and view the output file
az storage blob download \
  --account-name adfstoragedemo123 \
  --container-name destination \
  --name output.txt \
  --file output.txt && cat output.txt
```

---

# ğŸ¤– Method 2: Infrastructure as Code (Terraform)

## ğŸš€ Quick Start

```bash
# Clone or navigate to project
cd terraform/

# Initialize Terraform
terraform init

# Review deployment plan
terraform plan

# Deploy infrastructure
terraform apply -auto-approve
```

## ğŸ¯ What Gets Created Automatically

| Resource | Name | Description |
|----------|------|-------------|
| ğŸ“ Resource Group | `adf-rg` | Container for all resources |
| ğŸ’¾ Storage Account | `adfstoragedemo123` | Blob storage for data |
| ğŸ“¦ Source Container | `source` | Container with input.txt |
| ğŸ“¦ Destination Container | `destination` | Target for copied data |
| ğŸ­ Data Factory | `adf-simple-demo` | ETL orchestration service |
| ğŸ”— Linked Service | `AzureBlobStorage1` | Connection to storage |
| ğŸ“‹ Source Dataset | `SourceDataset` | Points to input.txt |
| ğŸ“‹ Sink Dataset | `SinkDataset` | Points to output.txt |
| ğŸ”„ Pipeline | `CopyBlobPipeline` | Copy activity workflow |
| â° Trigger | `DailyTrigger` | Scheduled execution (disabled) |
| ğŸ“„ Sample Data | `input.txt` | Auto-uploaded test file |

## âš™ï¸ Terraform Configuration

### ğŸ“ File Structure
```
terraform/
â”œâ”€â”€ main.tf                    # Infrastructure resources
â”œâ”€â”€ variables.tf              # Input parameters  
â”œâ”€â”€ outputs.tf               # Resource information
â”œâ”€â”€ terraform.tfvars.example # Configuration template
â”œâ”€â”€ deploy.sh               # Automated deployment
â”œâ”€â”€ README.md              # Terraform docs
â””â”€â”€ .gitignore            # Git exclusions
```

### ğŸ›ï¸ Customization

1. **Copy example config:**
   ```bash
   cp terraform.tfvars.example terraform.tfvars
   ```

2. **Edit values:**
   ```hcl
   resource_group_name   = "my-adf-rg"
   location              = "West US 2"
   storage_account_name  = "mystorageaccount123"
   data_factory_name     = "my-data-factory"
   environment          = "Production"
   ```

## ğŸ§ª Testing the Deployment

```bash
# Trigger pipeline manually
az datafactory pipeline create-run \
  --factory-name adf-simple-demo \
  --resource-group adf-rg \
  --name CopyBlobPipeline

# Monitor run status
az datafactory pipeline-run show \
  --factory-name adf-simple-demo \
  --resource-group adf-rg \
  --run-id <RUN_ID>

# Verify results
az storage blob list \
  --account-name adfstoragedemo123 \
  --container-name destination \
  --output table
```

## ğŸ§¹ Cleanup

```bash
# Destroy all resources
terraform destroy -auto-approve
```

---

# ğŸ“Š Comparison: Manual vs Terraform

| Aspect | Manual Setup | Terraform |
|--------|--------------|-----------|
| â±ï¸ **Setup Time** | 30-60 minutes | 5 minutes |
| ğŸ”„ **Repeatability** | Manual errors possible | 100% consistent |
| ğŸ“ **Documentation** | Screenshots/notes | Self-documenting code |
| ğŸ”§ **Customization** | Portal configuration | Variables & parameters |
| ğŸ¢ **Enterprise Ready** | Manual governance | Version controlled |
| ğŸ§¹ **Cleanup** | Manual deletion | One command |
| ğŸ“ˆ **Scaling** | Repeat manually | Copy/modify code |
| ğŸ›¡ï¸ **Best Practices** | Depends on user | Built-in standards |

---

## ğŸ§  Key Concepts Covered

- **Azure Data Factory**: Cloud ETL/ELT service
- **Linked Service**: Connection to external data stores
- **Dataset**: Pointer to specific data in a linked service
- **Pipeline**: Container for activities that perform tasks
- **Copy Activity**: Transfers data between source and sink
- **Trigger**: Mechanism to execute pipelines
- **Infrastructure as Code**: Automated, version-controlled deployments

---

## ğŸ¯ Learning Outcomes

After completing this project, you will understand:

1. **Manual Azure Data Factory Configuration**
   - Creating and configuring ADF components via Azure Portal
   - Understanding the relationship between datasets, pipelines, and activities
   - Monitoring and troubleshooting pipeline runs

2. **Infrastructure as Code with Terraform**
   - Automating Azure resource deployment
   - Managing infrastructure through version control
   - Implementing repeatable, scalable cloud architectures

3. **Best Practices**
   - Security considerations for data movement
   - Resource naming conventions and organization
   - Monitoring and operational excellence

---

## ğŸ“š Additional Resources

- ğŸ“– [Azure Data Factory Documentation](https://docs.microsoft.com/en-us/azure/data-factory/)
- ğŸ—ï¸ [Terraform Azure Provider](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs)
- ğŸ“ [Azure Data Factory Learning Path](https://docs.microsoft.com/en-us/learn/paths/data-integration-scale-azure-data-factory/)

---

## ğŸ“Œ One-Line Summary

> *Azure Data Factory is a cloud-based ETL service that enables automated data movement and transformation between various data sources using configurable pipelines, deployable through both manual configuration and Infrastructure as Code approaches.*