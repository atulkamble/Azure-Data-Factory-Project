# ðŸš€ Azure Data Factory 

## ðŸ§© Task Objective

**Copy a text file from Azure Blob Storage (Source) to another Blob container (Sink)** using **Azure Data Factory**.

---

## ðŸ—ï¸ Architecture (Simple)

```
Blob Storage (Source)  --->  Azure Data Factory  --->  Blob Storage (Destination)
```

---

## âœ… Prerequisites

* Azure Subscription
* Azure Storage Account
* Sample file (example: `input.txt`)
* Azure CLI installed (optional but recommended)

---

## ðŸ”¹ Step 1: Create Resource Group

```bash
az group create \
  --name adf-rg \
  --location eastus
```

---

## ðŸ”¹ Step 2: Create Storage Account

```bash
az storage account create \
  --name adfstoragedemo123 \
  --resource-group adf-rg \
  --location eastus \
  --sku Standard_LRS
```

---

## ðŸ”¹ Step 3: Create Containers

```bash
az storage container create \
  --account-name adfstoragedemo123 \
  --name source

az storage container create \
  --account-name adfstoragedemo123 \
  --name destination
```

---

## create file 
```
echo "Hello World" >> input.txt
cat ./input.txt
```
## ðŸ”¹ Step 4: Upload Sample File

```bash
az storage blob upload \
  --account-name adfstoragedemo123 \
  --container-name source \
  --name input.txt \
  --file input.txt
```

---
## install extension 
```
az extension add --name datafactory
```
## ðŸ”¹ Step 5: Create Azure Data Factory

```bash
az datafactory create \
  --resource-group adf-rg \
  --factory-name adf-simple-demo \
  --location eastus
```

---

## ðŸ”¹ Step 6: Open Azure Data Factory Studio

ðŸ‘‰ Azure Portal â†’ **Data Factories** â†’ `adf-simple-demo`
ðŸ‘‰ Click **Open Azure Data Factory Studio**

---

## ðŸ”¹ Step 7: Create Linked Service (Blob Storage)

### Linked Service JSON

```json
{
  "properties": {
    "type": "AzureBlobStorage",
    "typeProperties": {
      "connectionString": "DefaultEndpointsProtocol=https;AccountName=adfstoragedemo123;AccountKey=<KEY>;EndpointSuffix=core.windows.net"
    }
  }
}
```

---

## ðŸ”¹ Step 8: Create Datasets

### Source Dataset (Blob â€“ input.txt)

```json
{
  "properties": {
    "linkedServiceName": {
      "referenceName": "AzureBlobStorage1",
      "type": "LinkedServiceReference"
    },
    "type": "DelimitedText",
    "typeProperties": {
      "location": {
        "type": "AzureBlobStorageLocation",
        "container": "source",
        "fileName": "input.txt"
      },
      "columnDelimiter": ",",
      "firstRowAsHeader": true
    }
  }
}
```

### Sink Dataset (Blob â€“ output.txt)

```json
{
  "properties": {
    "linkedServiceName": {
      "referenceName": "AzureBlobStorage1",
      "type": "LinkedServiceReference"
    },
    "type": "DelimitedText",
    "typeProperties": {
      "location": {
        "type": "AzureBlobStorageLocation",
        "container": "destination",
        "fileName": "output.txt"
      }
    }
  }
}
```

---

## ðŸ”¹ Step 9: Create Pipeline with Copy Activity

### Pipeline JSON

```json
{
  "name": "CopyBlobPipeline",
  "properties": {
    "activities": [
      {
        "name": "CopyFromSourceToDestination",
        "type": "Copy",
        "inputs": [
          {
            "referenceName": "SourceDataset",
            "type": "DatasetReference"
          }
        ],
        "outputs": [
          {
            "referenceName": "SinkDataset",
            "type": "DatasetReference"
          }
        ],
        "typeProperties": {
          "source": {
            "type": "DelimitedTextSource"
          },
          "sink": {
            "type": "DelimitedTextSink"
          }
        }
      }
    ]
  }
}
```

---

## ðŸ”¹ Step 10: Validate & Publish

âœ” Click **Validate All**
âœ” Click **Publish**

---

## ðŸ”¹ Step 11: Trigger Pipeline

ðŸ‘‰ Click **Add Trigger â†’ Trigger Now**

---

## ðŸŽ‰ Output

* `input.txt` copied from **source** container
* `output.txt` appears in **destination** container

---

## ðŸ§  Key Concepts Covered

* Azure Data Factory
* Linked Service
* Dataset
* Pipeline
* Copy Activity
* Trigger

---

## ï¿½ Infrastructure as Code (Terraform)

This project now includes **Terraform Infrastructure as Code** for automated deployment!

### ðŸš€ Quick Deploy with Terraform

```bash
cd terraform/
terraform init
terraform plan
terraform apply
```

### ðŸŽ¯ What Terraform Creates

- âœ… Resource Group (`adf-rg`)
- âœ… Storage Account (`adfstoragedemo123`)
- âœ… Blob Containers (`source`, `destination`)
- âœ… Azure Data Factory (`adf-simple-demo`)
- âœ… Linked Service (Blob Storage connection)
- âœ… Datasets (Source and Sink for delimited text)
- âœ… Pipeline (Copy activity from source to destination)
- âœ… Scheduled Trigger (Daily at 9 AM, disabled by default)
- âœ… Sample Data (`input.txt` automatically uploaded)

### ðŸ“ Terraform Structure

```
terraform/
â”œâ”€â”€ main.tf              # Main infrastructure configuration
â”œâ”€â”€ variables.tf         # Input variables
â”œâ”€â”€ outputs.tf          # Output values
â”œâ”€â”€ terraform.tfvars.example  # Example variables
â”œâ”€â”€ deploy.sh           # Automated deployment script
â”œâ”€â”€ README.md           # Terraform documentation
â””â”€â”€ .gitignore         # Terraform-specific gitignore
```

### ðŸ§ª Test the Pipeline

```bash
# Trigger pipeline run
az datafactory pipeline create-run \
  --factory-name adf-simple-demo \
  --resource-group adf-rg \
  --name CopyBlobPipeline

# Check results
az storage blob list \
  --account-name adfstoragedemo123 \
  --container-name destination \
  --output table
```

---

## ï¿½ðŸ“Œ Interview / Training One-Line Explanation

> *Azure Data Factory is a cloud-based ETL service used to move and transform data between different data sources using pipelines.*

